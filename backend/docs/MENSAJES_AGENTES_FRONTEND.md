# Mensajes de Agentes en Frontend - SOLUCIÃ“N FINAL

## Problema Original

Los mensajes informativos de los agentes solo aparecÃ­an en la consola del backend pero no llegaban al frontend. El error era:

```
Error: Only plain objects, and a few built-ins, can be passed to Client Components from Server Components. 
Classes or null prototypes are not supported.
```

Este error se producÃ­a porque los objetos `AIMessage` de LangChain son clases que no pueden serializarse para React Server Components.

## SoluciÃ³n Implementada

### Resumen

La soluciÃ³n final combina dos elementos:

1. **Mensaje consolidado**: El Agente 3 genera UN mensaje final con toda la informaciÃ³n de los 3 agentes
2. **Nodo de respuesta final**: Usa el modelo de chat para generar eventos de streaming que el frontend captura
3. **SerializaciÃ³n JSON**: Convierte el `lastEvent` a JSON plano para evitar errores de serializaciÃ³n RSC

### Backend: Mensaje Consolidado + Nodo Final

#### 1. Agente 3 genera mensaje consolidado

El Agente 3 recopila informaciÃ³n de todos los agentes y genera un mensaje completo:

```python
def agente_3_calculador(state, config):
    # ... cÃ¡lculos ...
    
    # Mensaje consolidado con informaciÃ³n de los 3 agentes
    mensaje_consolidado = f"""ğŸ”„ **PROCESO COMPLETADO**

---

ğŸ“‹ **AGENTE 1: CLASIFICADOR**
- IntenciÃ³n detectada: **{intencion}**
- Productos solicitados: **{len(productos)}**

---

ğŸ” **AGENTE 2: BUSCADOR**
- Productos encontrados: **{len(productos_encontrados)}**
  âœ“ {producto.nombre} - {producto.precio}â‚¬

---

ğŸ’° **AGENTE 3: CALCULADOR**
- Subtotal: {subtotal}â‚¬
- **TOTAL: {total}â‚¬**

---

{ticket_completo}

Â¿Deseas confirmar la compra?
"""
    
    return Command(
        goto="respuesta_final",
        update={"final_result": mensaje_consolidado}
    )
```

#### 2. Nodo de Respuesta Final

Este nodo usa el modelo de chat para generar eventos de streaming:

```python
def nodo_respuesta_final(state, config):
    final_result = state.get("final_result", "No se pudo procesar la solicitud")
    
    # El modelo "repite" el mensaje, pero genera eventos de streaming
    model = ChatOpenAI(model="gpt-4o", temperature=0)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Devuelve el siguiente mensaje exactamente como estÃ¡."),
        ("human", "{mensaje}")
    ])
    
    chain = prompt | model
    response = chain.invoke({"mensaje": final_result}, config)
    
    return Command(goto="__end__", update={"messages": [response]})
```

**Â¿Por quÃ© funciona?**
- El modelo genera eventos `on_chat_model_stream` mientras procesa
- El frontend YA tiene un handler para estos eventos
- El modelo simplemente "repite" el mensaje (temperatura=0)
- Genera streaming real con efecto "typing"

### Frontend: SerializaciÃ³n JSON del lastEvent

#### Fix en `frontend/utils/server.tsx`

```typescript
// Serialize to plain JSON to avoid RSC serialization errors
const serializedValue = resolveValue ? JSON.parse(JSON.stringify(resolveValue)) : null;
resolve(serializedValue);
```

**Â¿Por quÃ© funciona?**
- `JSON.stringify()` convierte clases de LangChain a string
- `JSON.parse()` reconstruye como **plain object** (sin prototipos ni clases)
- Esto elimina el error "Only plain objects can be passed..."

## Flujo Completo

```
Usuario: "Quiero comprar leche"
    â†“
[AGENTE 1: CLASIFICADOR]
    â†’ Clasifica intenciÃ³n
    â†’ Extrae productos y cantidades
    â†“
[AGENTE 2: BUSCADOR]
    â†’ Busca en API de Mercadona
    â†’ Encuentra productos con precios
    â†“
[AGENTE 3: CALCULADOR]
    â†’ Calcula totales
    â†’ Genera ticket
    â†’ Crea mensaje consolidado con INFO DE LOS 3 AGENTES
    â†“
[NODO RESPUESTA FINAL]
    â†’ Usa modelo de chat para streaming
    â†’ Genera eventos on_chat_model_stream
    â†“
[FRONTEND]
    â†’ Captura eventos de streaming
    â†’ Muestra mensaje con efecto typing
    â†’ Serializa lastEvent a JSON plano
```

## Ejemplo de Mensaje Final

```
ğŸ”„ PROCESO COMPLETADO

---

ğŸ“‹ AGENTE 1: CLASIFICADOR
- IntenciÃ³n detectada: **compra**
- Productos solicitados: **1**

---

ğŸ” AGENTE 2: BUSCADOR
- Productos encontrados: **1**
  âœ“ Leche desnatada Hacendado - 0.84â‚¬

---

ğŸ’° AGENTE 3: CALCULADOR
- Subtotal: 0.84â‚¬
- **TOTAL: 0.84â‚¬**

---

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              MERCADONA - TICKET DE COMPRA             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Fecha: 06/10/2025 14:30:15

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PRODUCTOS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Leche desnatada Hacendado
   Botella 1L
   1 x 0.84â‚¬ = 0.84â‚¬

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RESUMEN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL A PAGAR:       0.84â‚¬
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Â¿Deseas confirmar la compra?
```

## Ventajas de Esta SoluciÃ³n

1. âœ… **Sin errores RSC**: La serializaciÃ³n JSON elimina objetos de clase
2. âœ… **Un solo mensaje consolidado**: MÃ¡s fÃ¡cil de leer para el usuario
3. âœ… **Streaming real**: El modelo genera eventos que el frontend captura
4. âœ… **Usa infraestructura existente**: No requiere cambios mayores
5. âœ… **InformaciÃ³n completa**: Muestra el trabajo de los 3 agentes
6. âœ… **Formato rico**: Markdown, emojis, ticket ASCII formateado
7. âœ… **Sin cambios complejos**: No requiere LangGraph SDK client

## Archivos Modificados

### Backend
- `backend/gen_ui_backend/chain.py`:
  - `agente_3_calculador()` - Genera mensaje consolidado con info de los 3 agentes
  - **Nuevo:** `nodo_respuesta_final()` - Usa modelo de chat para streaming
  - `create_multi_agent_graph()` - Agregado nodo `respuesta_final` al grafo
  - Todos los agentes ahora van a `respuesta_final` en lugar de `__end__`

### Frontend
- `frontend/utils/server.tsx`:
  - Agregada serializaciÃ³n JSON del `lastEvent` para evitar errores RSC
- `frontend/app/agent.tsx`:
  - Sin cambios (usa los handlers existentes)

## Testing

1. Iniciar el backend:
   ```bash
   cd backend
   python -m gen_ui_backend.server
   ```

2. Iniciar el frontend:
   ```bash
   cd frontend
   yarn dev
   ```

3. Abrir http://localhost:3000 y la consola del navegador (F12)

4. Enviar: "Quiero comprar leche"

5. Verificar:
   - âœ… NO hay error "Only plain objects..." en consola
   - âœ… Aparece UN mensaje consolidado con toda la informaciÃ³n
   - âœ… El mensaje muestra:
     - InformaciÃ³n del Agente 1 (clasificaciÃ³n)
     - InformaciÃ³n del Agente 2 (bÃºsqueda)
     - InformaciÃ³n del Agente 3 (cÃ¡lculo y ticket)
   - âœ… Tiene efecto de "typing" (streaming)

## Â¿Por QuÃ© get_stream_writer() No FuncionÃ³?

`get_stream_writer()` es la soluciÃ³n recomendada por LangGraph, PERO:

- `RemoteRunnable.streamEvents()` no captura eventos custom de LangGraph
- Se necesitarÃ­a usar `LangGraph SDK Client` con `stream_mode="custom"`
- Esto requiere cambios mayores en el cliente y configuraciÃ³n de LangServe
- La soluciÃ³n del nodo final es mÃ¡s simple y funciona con RemoteRunnable

## Referencias

- [LangGraph Streaming Documentation](https://langchain-ai.github.io/langgraph/how-tos/streaming/)
- [React Server Components Serialization](https://nextjs.org/docs/app/building-your-application/rendering/server-components#passing-props-from-server-to-client-components-serialization)
